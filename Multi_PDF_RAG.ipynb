{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi PDF RAG Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import fitz # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "\n",
    "# for efficient similarity search of vectors, which is useful for finding information quickly in large datasets\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/0*Q6HOo4_KyCnyFkbf.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Processing PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a user uploads one or more PDF files, the application reads each page of these documents and extracts the text, merging it into a single continuous string.\n",
    "\n",
    "Once the text is extracted, it is split into manageable chunks of 1000 characters each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_read_PyPDF2(pdf_doc):\n",
    "    text = ''\n",
    "    for pdf in pdf_doc:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def pdf_read_PyMuPDF(pdf_doc):\n",
    "    text = ''\n",
    "    for pdf in pdf_doc:\n",
    "        if type(pdf) == str: # read with path\n",
    "            pdf_reader = fitz.open(pdf)\n",
    "        else: # read with file object\n",
    "            pdf_reader = fitz.open(stream=pdf.read())\n",
    "            \n",
    "        for page_num in range(pdf_reader.page_count):\n",
    "            page = pdf_reader[page_num]\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H·ªì ƒêƒÉng Cao\n",
      "AI Engineer Intern\n",
      "OBJECTIVE\n",
      "I am eager to apply for the AI Engineer Intern position at TMA Tech Group. I aspire to \n",
      "apply my academic knowledge in real-world scenarios while learning and further  \n",
      "developing my skills under the guidance of industry experts. I have continuously been  \n",
      "reading science papers, learning and working on projects in this field every day. With \n",
      "enthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to \n",
      "the company‚Äô s success and simultaneously advance my professional growth in a creative\n",
      "and challenging environment.\n",
      "SKILLS\n",
      "Programming language: Python, SQL.  \n",
      "Data crawling:  Selenium, BeautifulSoup.\n",
      "Data pr ocessing: Numpy , Pandas, Excel.\n",
      "Machine learning:  Pytorch, Sciki\n"
     ]
    }
   ],
   "source": [
    "pdf_docs = ['./demo_pdf_file/CV-Ho-Dang-Cao.pdf', './demo_pdf_file/academic_transcript.pdf']\n",
    "print(pdf_read_PyPDF2([pdf_docs[0]])[:750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H·ªì ƒêƒÉng Cao\n",
      "AI Engineer Intern\n",
      "OBJECTIVE\n",
      "I am eager to apply for the AI Engineer Intern position at TMA Tech Group. I aspire to \n",
      "apply my academic knowledge in real-world scenarios while learning and further \n",
      "developing my skills under the guidance of industry experts. I have continuously been \n",
      "reading science papers, learning and working on projects in this field every day. With \n",
      "enthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to \n",
      "the company‚Äôs success and simultaneously advance my professional growth in a creative\n",
      "and challenging environment.\n",
      "SKILLS\n",
      "Programming language: Python, SQL. \n",
      "Data crawling: Selenium, BeautifulSoup.\n",
      "Data processing: Numpy, Pandas, Excel.\n",
      "Machine learning: Pytorch, Scikit-learn,\n"
     ]
    }
   ],
   "source": [
    "raw_text = pdf_read_PyMuPDF([pdf_docs[0]])\n",
    "print(raw_text[:750])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:`\n",
    "- PyMuPDF text is cleaner.\n",
    "- PyPDF2 code is cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_read_PyMuPDF([pdf_docs[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrade to read scanned PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanned PDFs typically contain images rather than text data that regular PDF text extraction libraries like PyPDF2, pdfplumber, and PyMuPDF can NOT handle. For scanned PDFs, use Optical Character Recognition (OCR) to extract text from images within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolved_pdf_read_PyMuPDF(pdf_doc: list, ocr_config: str):\n",
    "    text = ''\n",
    "    for pdf in pdf_doc:\n",
    "        if type(pdf) == str: # read with path\n",
    "            pdf_reader = fitz.open(pdf)\n",
    "        else: # read with file object\n",
    "            pdf_reader = fitz.open(stream=pdf.read())\n",
    "            \n",
    "        for page_num in range(pdf_reader.page_count):\n",
    "            page = pdf_reader[page_num]            \n",
    "            text_page = page.get_text()\n",
    "\n",
    "            if not text_page.strip(): # if no text => scanned file\n",
    "                pix = page.get_pixmap()\n",
    "                img = Image.open(io.BytesIO(pix.tobytes('png')))\n",
    "                text_page = pytesseract.image_to_string(img, config=ocr_config)\n",
    "\n",
    "            text += text_page\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ye\n",
      "\n",
      "=\n",
      "\n",
      "ose NATIONAL UNIVERSITY -HeMC\n",
      "\n",
      "SOCIALIST REPUBLIC OF VIETNAM\n",
      "\n",
      "i\n",
      "\n",
      "UNIVERSITY OF SCIENCE,\n",
      "\n",
      "Inde\n"
     ]
    }
   ],
   "source": [
    "ocr_config = r' --psm 11 --oem 3'\n",
    "\n",
    "raw_text = evolved_pdf_read_PyMuPDF([pdf_docs[1]], ocr_config)\n",
    "print(raw_text[:100])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H·ªì ƒêƒÉng Cao\\nAI Engineer Intern\\nOBJECTIVE\\nI am eager to apply for the AI Engineer Intern position at TMA Tech Group. I aspire to \\napply my academic knowledge in real-world scenarios while learning and further \\ndeveloping my skills under the guidance of industry experts. I have continuously been \\nreading science papers, learning and working on projects in this field every day. With \\nenthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to',\n",
       " 'reading science papers, learning and working on projects in this field every day. With \\nenthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to \\nthe company‚Äôs success and simultaneously advance my professional growth in a creative\\nand challenging environment.\\nSKILLS\\nProgramming language: Python, SQL. \\nData crawling: Selenium, BeautifulSoup.\\nData processing: Numpy, Pandas, Excel.\\nMachine learning: Pytorch, Scikit-learn, Statsmodel, Tensorflow, Keras.',\n",
       " 'SKILLS\\nProgramming language: Python, SQL. \\nData crawling: Selenium, BeautifulSoup.\\nData processing: Numpy, Pandas, Excel.\\nMachine learning: Pytorch, Scikit-learn, Statsmodel, Tensorflow, Keras.\\nBig data: Hadoop, Pyspark. \\nDataBase: MSSQL, MongoDB \\n\\ue11b\\n097 2367 154\\n‚úâ\\ndangcaoho151202@gmail.com\\nüåê\\nhttps://github.com/hodangcao \\nEDUCATION\\nUniversity of Science-VNUHCM       2020 - 2024\\nINFORMATION TECNOLOGY\\n‚Ä¢ Major: Data Science (High-Quality Program)\\n\\ufeff‚Ä¢ GPA: 8.18/10',\n",
       " 'AWARDS & CERTIFICATES\\nSemester 3 - 2020-2021 \\nExcellent achievement in academic semesters.\\nOctober 7, 2024\\nVSTEP certificate: 6.5/10\\nLASTEST PROJECTS\\nLARGE LANGUAGE MODELS \\n(LLMs) SYSTEM\\nTeam size - 1\\nGitHub \\nhttps://github.com/HoDangCao/LL\\nMs.git \\nPROJECT DESCRIPTION \\nBuilding components of a LLMs system step by step from cratch:\\n‚Ä¢ Retrieval-Augmented Generation (RAG).\\n‚Ä¢ Chain of Thought (CoT).\\n‚Ä¢ Llama 3 model.\\nTechnology \\nPytorch, Numpy, Pandas, Spacy, Sklearn, Transformers, Matplotlib.',\n",
       " \"‚Ä¢ Retrieval-Augmented Generation (RAG).\\n‚Ä¢ Chain of Thought (CoT).\\n‚Ä¢ Llama 3 model.\\nTechnology \\nPytorch, Numpy, Pandas, Spacy, Sklearn, Transformers, Matplotlib. \\nFOOD DEMAND PROJECT \\nTeam size - 1 \\nGitHub \\nhttps://github.com/HoDangCao/Foo\\nd-Demand.git \\nPROJECT DESCRIPTION \\nThe project aims to analyze Genpact's demand dataset (mastering EDA) and build algorithms (such as \\nGBM, XGBoost, LSTM, etc) to predict future food demand (ML). By the project's end, the algorithm\",\n",
       " \"The project aims to analyze Genpact's demand dataset (mastering EDA) and build algorithms (such as \\nGBM, XGBoost, LSTM, etc) to predict future food demand (ML). By the project's end, the algorithm \\nwill be deployed as an API using FastAPI, allowing it to be integrated into web or mobile applications \\nfor real-world use.\\nTechnology \\nNumpy, Pandas, Matplotlib, Seaborn, Statsmodel, Tensorflow, FastAPI. \\nMULTI-OBJECT \\nCLASSIFICATION USING \\nDEEP LEARNING MODEL\\nTeam size - 2\\nGitHub\",\n",
       " 'for real-world use.\\nTechnology \\nNumpy, Pandas, Matplotlib, Seaborn, Statsmodel, Tensorflow, FastAPI. \\nMULTI-OBJECT \\nCLASSIFICATION USING \\nDEEP LEARNING MODEL\\nTeam size - 2\\nGitHub \\nhttps://github.com/HoDangCao/mult\\ni-object-classification-based-on-\\ndeep-learning-model.git \\nPROJECT DESCRIPTION \\nPropose solutions and models to address challenges in real-world multi-object image classification.\\nMy Responsibility: Investigate scientific papers related to multi-object classification tasks; Analyze',\n",
       " 'My Responsibility: Investigate scientific papers related to multi-object classification tasks; Analyze \\nreal-world datasets to indentify issues that need to be resolved; Build image reprocessing techniques to \\neliminate unnecessary background elements; Implement, modify and enhance Only-Positive-Label \\nmodel and combine with C-Tran models.\\nTechnology\\nPytorch, Numpy, Pandas, Streamlit, Scikit-learn. \\n¬© topcv.vn']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = get_chunks(raw_text)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Searchable Text Database and Making Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application turns text chunks into vectors and saves these vectors locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SpacyEmbeddings(model_name='en_core_web_sm')\n",
    "\n",
    "def vector_store(text_chunks):\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local('faiss_db')\n",
    "    \n",
    "vector_store(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_extractor\n",
      "This tool is to give answer to queries from the pdf\n"
     ]
    }
   ],
   "source": [
    "new_db = FAISS.load_local('faiss_db', embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = new_db.as_retriever(search_kwargs={\"k\": 1}) # top_k=1 will return only the top chunk.\n",
    "retriever_chain = create_retriever_tool(retriever, 'pdf_extractor', 'This tool is to give answer to queries from the pdf')\n",
    "print(retriever_chain.name)\n",
    "print(retriever_chain.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Programming language: Python, SQL.  \\nData crawling:  Selenium, BeautifulSoup.\\nData pr ocessing: Numpy , Pandas, Excel.\\nMachine learning:  Pytorch, Scikit-learn, Statsmodel, T ensorflow , Keras.\\nBig data: Hadoop, Pyspark.  \\nDataBase : MSSQL, MongoDB  \\ue11b 097 2367 154\\n‚úâ dangcaoho151202@gmail.com\\n\\uf0ac https://github.com/hodangcao  \\nEDUCA TION\\nUniversity of Science-VNUHCM       2020 -2024\\nINFORMA TION TECNOLOGY\\n‚Ä¢ Major: Data Science (High-Quality Program)\\n  ‚Ä¢ GPA: 8.18/10 \\n \\nAWARDS & CER TIFICA TES'),\n",
       " Document(metadata={}, page_content='reading science papers, learning and working on projects in this field every day. With \\nenthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to \\nthe company‚Äô s success and simultaneously advance my professional growth in a creative\\nand challenging environment.\\nSKILLS\\nProgramming language: Python, SQL.  \\nData crawling:  Selenium, BeautifulSoup.\\nData pr ocessing: Numpy , Pandas, Excel.\\nMachine learning:  Pytorch, Scikit-learn, Statsmodel, T ensorflow , Keras.'),\n",
       " Document(metadata={}, page_content='eliminate unnecessary background elements; Implement, modify and enhance Only-Positive-Label  \\nmodel and combine with C-T ran models.\\nTechnology\\nPytorch, Numpy , Pandas, Streamlit, Scikit-learn.  ¬© topcv.vn'),\n",
       " Document(metadata={}, page_content=\"The project aims to analyze Genpact's demand dataset (mastering EDA) and build algorithms (such as \\nGBM, XGBoost, LSTM, etc) to predict future food demand (ML). By the project's end, the algorithm  \\nwill be deplo yed as an API using FastAPI, allowing it to be integrated into web or mobile applications  \\nfor real-world use.\\nTechnology  \\nNumpy , Pandas, Matplotlib, Seaborn, Statsmodel, T ensorflow , FastAPI.  \\nMUL TI-OBJECT  \\nCLASSIFICA TION USING  \\nDEEP LEARNING MODEL\\nTeam size -2\\nGitHub\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = ['show GPA','list the projects titles','December']\n",
    "new_db.similarity_search(queries[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangc\\AppData\\Local\\Temp\\ipykernel_2908\\1213172964.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(queries[2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Programming language: Python, SQL.  \\nData crawling:  Selenium, BeautifulSoup.\\nData pr ocessing: Numpy , Pandas, Excel.\\nMachine learning:  Pytorch, Scikit-learn, Statsmodel, T ensorflow , Keras.\\nBig data: Hadoop, Pyspark.  \\nDataBase : MSSQL, MongoDB  \\ue11b 097 2367 154\\n‚úâ dangcaoho151202@gmail.com\\n\\uf0ac https://github.com/hodangcao  \\nEDUCA TION\\nUniversity of Science-VNUHCM       2020 -2024\\nINFORMA TION TECNOLOGY\\n‚Ä¢ Major: Data Science (High-Quality Program)\\n  ‚Ä¢ GPA: 8.18/10 \\n \\nAWARDS & CER TIFICA TES'),\n",
       " Document(metadata={}, page_content='reading science papers, learning and working on projects in this field every day. With \\nenthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to \\nthe company‚Äô s success and simultaneously advance my professional growth in a creative\\nand challenging environment.\\nSKILLS\\nProgramming language: Python, SQL.  \\nData crawling:  Selenium, BeautifulSoup.\\nData pr ocessing: Numpy , Pandas, Excel.\\nMachine learning:  Pytorch, Scikit-learn, Statsmodel, T ensorflow , Keras.'),\n",
       " Document(metadata={}, page_content='eliminate unnecessary background elements; Implement, modify and enhance Only-Positive-Label  \\nmodel and combine with C-T ran models.\\nTechnology\\nPytorch, Numpy , Pandas, Streamlit, Scikit-learn.  ¬© topcv.vn'),\n",
       " Document(metadata={}, page_content=\"The project aims to analyze Genpact's demand dataset (mastering EDA) and build algorithms (such as \\nGBM, XGBoost, LSTM, etc) to predict future food demand (ML). By the project's end, the algorithm  \\nwill be deplo yed as an API using FastAPI, allowing it to be integrated into web or mobile applications  \\nfor real-world use.\\nTechnology  \\nNumpy , Pandas, Matplotlib, Seaborn, Statsmodel, T ensorflow , FastAPI.  \\nMUL TI-OBJECT  \\nCLASSIFICA TION USING  \\nDEEP LEARNING MODEL\\nTeam size -2\\nGitHub\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(queries[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Programming language: Python, SQL.  \\nData crawling:  Selenium, BeautifulSoup.\\nData pr ocessing: Numpy , Pandas, Excel.\\nMachine learning:  Pytorch, Scikit-learn, Statsmodel, T ensorflow , Keras.\\nBig data: Hadoop, Pyspark.  \\nDataBase : MSSQL, MongoDB  \\ue11b 097 2367 154\\n‚úâ dangcaoho151202@gmail.com\\n\\uf0ac https://github.com/hodangcao  \\nEDUCA TION\\nUniversity of Science-VNUHCM       2020 -2024\\nINFORMA TION TECNOLOGY\\n‚Ä¢ Major: Data Science (High-Quality Program)\\n  ‚Ä¢ GPA: 8.18/10 \\n \\nAWARDS & CER TIFICA TES\\n\\nreading science papers, learning and working on projects in this field every day. With \\nenthusiasm and a growth-oriented mindset, I am confident in my ability to contribute to \\nthe company‚Äô s success and simultaneously advance my professional growth in a creative\\nand challenging environment.\\nSKILLS\\nProgramming language: Python, SQL.  \\nData crawling:  Selenium, BeautifulSoup.\\nData pr ocessing: Numpy , Pandas, Excel.\\nMachine learning:  Pytorch, Scikit-learn, Statsmodel, T ensorflow , Keras.\\n\\neliminate unnecessary background elements; Implement, modify and enhance Only-Positive-Label  \\nmodel and combine with C-T ran models.\\nTechnology\\nPytorch, Numpy , Pandas, Streamlit, Scikit-learn.  ¬© topcv.vn\\n\\nThe project aims to analyze Genpact's demand dataset (mastering EDA) and build algorithms (such as \\nGBM, XGBoost, LSTM, etc) to predict future food demand (ML). By the project's end, the algorithm  \\nwill be deplo yed as an API using FastAPI, allowing it to be integrated into web or mobile applications  \\nfor real-world use.\\nTechnology  \\nNumpy , Pandas, Matplotlib, Seaborn, Statsmodel, T ensorflow , FastAPI.  \\nMUL TI-OBJECT  \\nCLASSIFICA TION USING  \\nDEEP LEARNING MODEL\\nTeam size -2\\nGitHub\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain.invoke(queries[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Conversational AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **AI Configuration**: The app sets up a conversational AI to answer questions based on the PDF content it has processed.\n",
    "- **Conversation Chain**: The AI uses a set of prompts to understand the context and provide accurate responses to user queries. If the answer to a question isn‚Äôt available in the text, the AI is programmed to respond with ‚Äúanswer is not available in the context,‚Äù ensuring that users do not receive incorrect information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dangc\\AppData\\Local\\Temp\\ipykernel_2908\\2118534138.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generation_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer locally from Hugging Face\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Create the Hugging Face pipeline for conversational tasks with specified max_length\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    # max_length=1024,\n",
    "    max_new_tokens=200, \n",
    "    pad_token_id=50256\n",
    ")\n",
    "\n",
    "# Wrap the pipeline with HuggingFacePipeline to integrate with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in provided context just say, \"answer is not available in the context\", don't provide the wrong answer\"\"\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant with access to the following tool: {tool_name}. {tool_description}\"),\n",
    "    (\"user\", \"Given document: {related_chunk}\"), \n",
    "    (\"user\", \"{query}\"),\n",
    "    (\"assistant\", \"Let me think...\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My main goal for this  august position is to learn how the project is progressing and become a part of  \n",
      "the human side of the   ecosystem. I want to assist    ¬†other           ¬†I. e. find new                   \n",
      "I am already an engineer but I need to keep up with             , and I want to provide my advices in               \n",
      "How does the project will be organized              You can find           \n",
      "The first step of  ¬†      ÓÄ¢ a good           ÓÄÖ t\n"
     ]
    }
   ],
   "source": [
    "def get_conversational_chain(retriever_chain, query):\n",
    "    tool_response = retriever_chain.invoke({'query': query})\n",
    "    \n",
    "    # Check if tool_response is a string or dict and access accordingly\n",
    "    if isinstance(tool_response, dict):\n",
    "        agent_scratchpad = tool_response.get('output', \"No output found.\")\n",
    "    else:\n",
    "        agent_scratchpad = tool_response\n",
    "\n",
    "    # Combine tool response and context with user input and prompt\n",
    "    formatted_prompt = prompt.format(\n",
    "        tool_name=retriever_chain.name,\n",
    "        tool_description=retriever_chain.description,\n",
    "        related_chunk=agent_scratchpad,\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    # Call the LLM with the formatted prompt\n",
    "    response = llm.invoke(formatted_prompt, temperature=0.7)\n",
    "    return response[len(formatted_prompt):].strip()\n",
    "\n",
    "query = 'list the projects titles'\n",
    "response_without_context = get_conversational_chain(retriever_chain, query)\n",
    "print(response_without_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker build -t pdf_rag .\n",
    "# !docker run -it pdf_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Building a Multi PDF RAG Chatbot: Langchain, Streamlit with code](https://blog.gopenai.com/building-a-multi-pdf-rag-chatbot-langchain-streamlit-with-code-d21d0a1cf9e5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
